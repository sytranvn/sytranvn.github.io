<!doctype html><html lang=en><head><title>Shrinking Your AI Deployments: Optimizing PyTorch/CUDA Docker Images with uv ¬∑ sytranvn.dev</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Sy Tran"><meta name=description content="A guide to optimizing PyTorch/CUDA Docker images by leveraging
pytorch/pytorch base images and uv for efficient dependency management.
"><meta name=keywords content="blog,developer,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Shrinking Your AI Deployments: Optimizing PyTorch/CUDA Docker Images with uv"><meta name=twitter:description content="A guide to optimizing PyTorch/CUDA Docker images by leveraging
pytorch/pytorch base images and uv for efficient dependency management."><meta property="og:url" content="https://sytranvn.dev/posts/optimizing-pytorch-cuda-docker-image-with-uv/"><meta property="og:site_name" content="sytranvn.dev"><meta property="og:title" content="Shrinking Your AI Deployments: Optimizing PyTorch/CUDA Docker Images with uv"><meta property="og:description" content="A guide to optimizing PyTorch/CUDA Docker images by leveraging
pytorch/pytorch base images and uv for efficient dependency management."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-08T16:10:44+07:00"><meta property="article:modified_time" content="2025-12-08T16:10:44+07:00"><meta property="article:tag" content="PyTorch"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="Docker"><meta property="article:tag" content="Uv"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="MLOps"><link rel=canonical href=https://sytranvn.dev/posts/optimizing-pytorch-cuda-docker-image-with-uv/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.4b392a85107b91dbdabc528edf014a6ab1a30cd44cafcd5325c8efe796794fca.css integrity="sha256-SzkqhRB7kdvavFKO3wFKarGjDNRMr81TJcjv55Z5T8o=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/custom.min.36eb58d02db7d3831e8ec30480fa209ae93627f039c1c577e7fdeece45784a20.css integrity="sha256-NutY0C2304MejsMEgPogmuk2J/A5wcV35/3uzkV4SiA=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://sytranvn.dev/>sytranvn.dev
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/now/>Now</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/i-heart-oss/>I ‚ù§Ô∏è OSS</a></li><li class=navigation-item><a class=navigation-link href=/accomplishments/>Accomplishments</a></li><li class=navigation-item><a class=navigation-link href=/resume.pdf>Resume</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/vi/>üáªüá≥</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://sytranvn.dev/posts/optimizing-pytorch-cuda-docker-image-with-uv/>Shrinking Your AI Deployments: Optimizing PyTorch/CUDA Docker Images with uv</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2025-12-08T16:10:44+07:00>December 8, 2025
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
4-minute read</span></div><div class=authors><i class="fa-solid fa-user" aria-hidden=true></i>
<a href=/authors/your-name/>Your Name</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/pytorch/>PyTorch</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/cuda/>CUDA</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/docker/>Docker</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/uv/>Uv</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/optimization/>Optimization</a>
</span><span class=separator>‚Ä¢</span>
<span class=tag><a href=/tags/mlops/>MLOps</a></span></div></div><script id=diffblog-plugin-script async src=https://diff.blog/static/js/diffblog_plugin_v1.js></script><script>document.getElementById("diffblog-plugin-script").addEventListener("load",function(){DiffBlog("4at5qx7n0j5yhd86rqu6gyotkgn4o17hu8wn6xis54lwd692ez")})</script></header><div class=post-content>A guide to optimizing PyTorch/CUDA Docker images by leveraging
pytorch/pytorch base images and uv for efficient dependency management.<h2 id=shrinking-your-ai-deployments-optimizing-pytorchcuda-docker-images-with-uv>Shrinking Your AI Deployments: Optimizing PyTorch/CUDA Docker Images with <code>uv</code>
<a class=heading-link href=#shrinking-your-ai-deployments-optimizing-pytorchcuda-docker-images-with-uv><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Deploying AI models in production often means packaging them into Docker
containers. While convenient, these images can quickly grow to colossal sizes,
especially when dealing with deep learning frameworks like PyTorch and their
CUDA dependencies. Large images lead to slower deployments, increased storage
costs, and longer CI/CD cycles.</p><h3 id=the-problem-with-large-images>The Problem with Large Images
<a class=heading-link href=#the-problem-with-large-images><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Many developers build their Docker images from generic base images (like
<code>ubuntu</code> or <code>python</code>) and then manually install PyTorch, CUDA, and other
libraries. This often results in:</p><ul><li><strong>Duplicate Dependencies:</strong> Installing CUDA drivers and PyTorch from
scratch often pulls in many system libraries that might already be optimized or
present in specialized base images.</li><li><strong>Version Mismatches:</strong> Managing CUDA, cuDNN, PyTorch, and Python versions
manually can be a nightmare, leading to runtime errors.</li><li><strong>Bloated Layers:</strong> Each installation step adds a new layer, increasing the
final image size unnecessarily.</li></ul><ul><li><strong>Huge Docker image</strong>: Your image is about 10GB after installing torch and cuda.</li></ul><h3 id=the-solution-leverage-pytorchpytorch-and-uv>The Solution: Leverage <code>pytorch/pytorch</code> and <code>uv</code>
<a class=heading-link href=#the-solution-leverage-pytorchpytorch-and-uv><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>To combat this, we propose a two-pronged strategy:</p><ol><li><strong>Start with an optimized base image:</strong> <code>pytorch/pytorch</code> images are
pre-configured with PyTorch, CUDA, cuDNN, and often MKL/OpenBLAS, ensuring
an optimized and compatible environment.</li><li><strong>Efficient dependency management with <code>uv</code>:</strong> Use <code>uv</code> (the fast Python
package installer and resolver) to manage your project&rsquo;s specific Python
dependencies, critically <em>excluding</em> <code>torch</code> to avoid redundant installations.</li></ol><h3 id=step-by-step-optimization>Step-by-Step Optimization
<a class=heading-link href=#step-by-step-optimization><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Let&rsquo;s walk through how to build a lean, optimized PyTorch/CUDA Docker image.</p><h4 id=1-choose-your-pytorchpytorch-base-image>1. Choose Your <code>pytorch/pytorch</code> Base Image
<a class=heading-link href=#1-choose-your-pytorchpytorch-base-image><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>The <code>pytorch/pytorch</code> Docker Hub repository offers a variety of images tailored
for different CUDA versions and Python environments. Select one that matches
your requirements. For example, for CUDA 11.8 and Python 3.10:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=w> </span><span class=s>pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime</span><span class=err>
</span></span></span></code></pre></div><p><strong>Why <code>pytorch/pytorch</code>?</strong> These images are maintained by the PyTorch team,
providing:</p><ul><li>Pre-installed PyTorch, CUDA toolkit, and cuDNN.</li><li>Optimized configurations for performance.</li><li>Guaranteed compatibility between PyTorch and its underlying CUDA libraries.</li><li>Reduced build time for these core components.</li></ul><h4 id=2-introduce-uv-for-fast-and-lean-dependency-management>2. Introduce <code>uv</code> for Fast and Lean Dependency Management
<a class=heading-link href=#2-introduce-uv-for-fast-and-lean-dependency-management><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p><code>uv</code> is a modern, extremely fast Python package installer and resolver. It&rsquo;s an
excellent replacement for <code>pip</code> and can significantly speed up your Docker
builds while keeping image sizes small.</p><p>First, install <code>uv</code> in your Docker image. Then, use it to export your
project&rsquo;s dependencies to <code>requirements.txt</code>. The crucial step here is to
tell <code>uv</code> <em>not</em> to include <code>torch</code> again, as it&rsquo;s already provided by the base
image.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>RUN</span> pip install --no-cache-dir uv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml uv.lock .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> uv <span class=nb>export</span> --format requirements-txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --no-hashes <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --no-dev <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --prune torch <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --prune torchvision <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -o requirements.txt<span class=err>
</span></span></span></code></pre></div><p><strong>A note on <code>requirements.txt</code> and <code>torch</code>:</strong></p><p>Ideally, if your <code>pytorch/pytorch</code> base image provides the <code>torch</code> package you
need, you should <strong>remove <code>torch</code> from your <code>requirements.txt</code></strong> file when
building this Docker image. This makes the <code>uv</code> step clean and simple, avoiding
any potential conflicts or redundant downloads.</p><p><strong>Example <code>requirements.txt</code> (without <code>torch</code>):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>numpy==1.26.2
</span></span><span class=line><span class=cl>pandas==2.1.3
</span></span><span class=line><span class=cl>scikit-learn==1.3.2
</span></span><span class=line><span class=cl>transformers==4.35.2
</span></span><span class=line><span class=cl># etc.
</span></span></code></pre></div><h4 id=full-dockerfile-example>Full <code>Dockerfile</code> Example
<a class=heading-link href=#full-dockerfile-example><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-dockerfile data-lang=dockerfile><span class=line><span class=cl><span class=k>FROM</span><span class=w> </span><span class=s>python:3.12-slim-trixie</span><span class=w> </span><span class=k>AS</span><span class=w> </span><span class=s>builder</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/build</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install --no-cache-dir uv<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> pyproject.toml pyproject.toml<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># NOTE: excluding torch and torchvision since runtime image</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># already installed</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> uv <span class=nb>export</span> --format requirements-txt <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --no-hashes <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --no-dev <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --prune torch <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --prune torchvision <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    -o requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Use PyTorch official image with CUDA support (much smaller than building from scratch)</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>FROM</span><span class=w> </span><span class=s>pytorch/pytorch:2.6.0-cuda12.4-cudnn9-runtime</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=c># Set environment variables</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>PATH</span><span class=o>=</span>/usr/local/cuda/bin:<span class=nv>$PATH</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>ENV</span> <span class=nv>LD_LIBRARY_PATH</span><span class=o>=</span>/usr/local/cuda/lib64:<span class=nv>$LD_LIBRARY_PATH</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>WORKDIR</span><span class=w> </span><span class=s>/app</span><span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> --from<span class=o>=</span>builder /build/requirements.txt requirements.txt<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>RUN</span> pip install -r requirements.txt --no-cache-dir<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>COPY</span> src .<span class=err>
</span></span></span><span class=line><span class=cl><span class=err>
</span></span></span><span class=line><span class=cl><span class=err></span><span class=k>CMD</span> <span class=p>[</span><span class=s2>&#34;python3&#34;</span><span class=p>,</span> <span class=s2>&#34;main.py&#34;</span><span class=p>]</span><span class=err>
</span></span></span></code></pre></div><h3 id=benefits-of-this-approach>Benefits of this Approach:
<a class=heading-link href=#benefits-of-this-approach><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ul><li><strong>Significantly Smaller Image Sizes:</strong> By avoiding duplicate installations
of PyTorch and CUDA dependencies, your final image will be much leaner.</li><li><strong>Faster Builds:</strong> By using a pre-built base image, the heavy layers of Torch and CUDA are already cached, drastically improving build times. Additionally, <code>uv</code>&rsquo;s speed and the fact that only lighter, project-specific dependencies need to be reinstalled upon changes further accelerate the build process.</li><li><strong>Reduced Conflicts:</strong> Relying on the <code>pytorch/pytorch</code> image for the core
ML stack minimizes the risk of version conflicts between CUDA, cuDNN, and
PyTorch.</li><li><strong>Easier Maintenance:</strong> The PyTorch team handles the heavy lifting of
optimizing the base environment. You just manage your application-specific
libraries.</li></ul><h3 id=conclusion>Conclusion:
<a class=heading-link href=#conclusion><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><p>Optimizing Docker images for deep learning is a critical step towards efficient
MLOps. By combining the power of the pre-optimized <code>pytorch/pytorch</code> base
images with the blazing-fast and dependency-aware <code>uv</code> installer, you can
drastically reduce your image sizes, accelerate your CI/CD pipelines, and
streamline your AI model deployments. Give it a try and experience the
difference!</p></div><footer></footer></article></section></div><footer class=footer><section class=container>¬©
2021 -
2025
Sy Tran
¬∑
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>